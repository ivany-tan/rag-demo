{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4523d453-48f7-4dc7-a687-987cb9d8ce7c",
   "metadata": {},
   "source": [
    "# Raw Document Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431be288-3c0a-4498-8bc5-4461fe9efe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 8085\n",
      "Chinese characters: 6433 (≈ words)\n",
      "English words: 24\n",
      "Approximate total words: 6457\n"
     ]
    }
   ],
   "source": [
    "def word_count(file_name: str) -> None:\n",
    "    # More accurate word count for Chinese\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count Chinese characters (each Chinese char ≈ 1 word)\n",
    "    chinese_chars = len([c for c in content if '\\u4e00' <= c <= '\\u9fff'])\n",
    "    # Count English words\n",
    "    english_words = len([w for w in content.split() if any('a' <= c.lower() <= 'z' for c in w)])\n",
    "    \n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"Chinese characters: {chinese_chars} (≈ words)\")\n",
    "    print(f\"English words: {english_words}\")\n",
    "    print(f\"Approximate total words: {chinese_chars + english_words}\")\n",
    "\n",
    "word_count(\"cn1.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9e1bc-b4bd-48b1-81bb-434cfe28a29c",
   "metadata": {},
   "source": [
    ">**5 chunks is actually good enough for a short story (i.e., 1,897 chars).** (i) Fast retrieval speed as only 5 embeddings to search through; And (ii) context preservation since each chunk has ~400 chars - enough context for a complete scene.\n",
    ">\n",
    ">**You'd want 10-20+ chunks if** (i) Document is 10,000+ characters (long article/book); (ii) Very specific factual queries requiring granular retrieval; (iii) Multiple topics in one document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516fda4-83b1-4866-8ed8-f7dac6fbe80f",
   "metadata": {},
   "source": [
    "# Splitting and Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fac282-e1c2-4a8d-bd5f-ebd704c2811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_into_chunks(\n",
    "    doc_file: str, \n",
    "    chunk_size: int = 800, \n",
    "    chunk_overlap: int = 150\n",
    ") -> List[str]:\n",
    "    with open(doc_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \"\\n\", \"。\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(content)\n",
    "\n",
    "chunks = split_into_chunks(\"cn1.md\")\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc280e-38d5-4c9f-bb8f-e1bf35dc850b",
   "metadata": {},
   "source": [
    "# Indexing and Storage\n",
    "Embedding Model Choice:\n",
    "- `shibing624/text2vec-base-chinese` (good for Chinese and used in `rag0`)\n",
    "- `BAAI/bge-base-zh-v1.5` (better Chinese performance)\n",
    "- `moka-ai/m3e-base` (multilingual Chinese-English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a827e8-b9f9-4d4e-a167-862450fa0720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07750a6c3f343cb806e6af1f9a6effb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted old collection\n",
      "Created collection with metadata: {'hnsw:space': 'cosine'}\n",
      "Generated 15 embeddings\n",
      "Embedding dimension: 768\n",
      "Saved 15 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "# ======================================== \n",
    "# Initialize models\n",
    "# ========================================\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Create ChromaDB with correct settings\n",
    "# ========================================\n",
    "def create_db():\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "    # If you want to GUARANTEE cosine space, delete+recreate:\n",
    "    try:\n",
    "        client.delete_collection(name=\"default\")\n",
    "        print(\"Deleted old collection\")\n",
    "    except Exception as e:\n",
    "        # ok if it doesn't exist; still good to know unexpected errors\n",
    "        print(f\"(delete_collection) {e}\")\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=\"default\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    print(f\"Created collection with metadata: {collection.metadata}\")\n",
    "    return collection\n",
    "\n",
    "chromadb_collection = create_db()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Embed\n",
    "# ========================================\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    emb = embedding_model.encode(chunk)  # numpy array\n",
    "    return emb.tolist()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Store with metadata\n",
    "# ========================================\n",
    "def save_embeddings(\n",
    "    collection,\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    "    source_file: str,\n",
    ") -> None:\n",
    "    if not chunks:\n",
    "        raise ValueError(\"chunks is empty\")\n",
    "\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(f\"chunks ({len(chunks)}) and embeddings ({len(embeddings)}) length mismatch\")\n",
    "    \n",
    "    if not source_file:\n",
    "        return ValueError(\"file is empty\")\n",
    "\n",
    "    # Use stable IDs that won't collide across different files\n",
    "    ids = [f\"{source_file}:{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": source_file,\n",
    "            \"chunk_length\": len(chunk),\n",
    "            \"chunk_index\": i,\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Use upsert so reruns don't explode on duplicate ids\n",
    "    collection.upsert(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# embeddings\n",
    "embeddings = [embed_chunk(c) for c in chunks]\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "save_embeddings(chromadb_collection, chunks, embeddings, \"cn1.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae254c19-7af1-4f4b-9824-6cfcc7b9119a",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bfcee1-6e7d-4ce0-a351-ddf24acdff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5, score_threshold = None):\n",
    "    query_embedding = embed_chunk(query)\n",
    "    \n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"No results found!\")\n",
    "        return []\n",
    "    \n",
    "    retrieved = []\n",
    "    for i, (doc, dist, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    )):\n",
    "        if score_threshold is None or dist <= score_threshold:\n",
    "            retrieved.append({\n",
    "                'document': doc,\n",
    "                'distance': dist,\n",
    "                'similarity': 1 / (1 + dist),\n",
    "                'metadata': meta,\n",
    "                'rank': i\n",
    "            })\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved)}/{top_k} chunks\")\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36de35d-a389-4548-83ed-959db76c7bfd",
   "metadata": {},
   "source": [
    "## Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640899dd-c31f-4afd-be9a-d63b7e86c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n",
      "\n",
      "======================================================================\n",
      "Results for  哈利波特用了什么魔法打败了索伦？\n",
      "======================================================================\n",
      "\n",
      "Rank 1:\n",
      "  Distance: 0.422\n",
      "  Similarity: 0.703\n",
      "  Text: ## 第五章：魔法的融合\n",
      "\n",
      "临战前夜，霍格沃茨的钟楼熄灯，取而代之的是城堡上空缓缓旋转的守护星光魔阵。夜色中笼罩着一股难以名状的寂静，仿佛整个世界都屏息等待。\n",
      "...\n",
      "\n",
      "Rank 2:\n",
      "  Distance: 0.439\n",
      "  Similarity: 0.695\n",
      "  Text: # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓...\n",
      "\n",
      "Rank 3:\n",
      "  Distance: 0.457\n",
      "  Similarity: 0.686\n",
      "  Text: 火焰之眼骤然收缩、崩塌，在空中发出最后一声爆响后，化作万千光点飘散。半兽人军团仿佛失去灵魂的傀儡，纷纷倒地，化为虚无。\n",
      "\n",
      "天边第一道金光破晓。\n",
      "\n",
      "哈利缓缓放下魔...\n",
      "\n",
      "Rank 4:\n",
      "  Distance: 0.464\n",
      "  Similarity: 0.683\n",
      "  Text: “准备好了？”赫敏看着他。\n",
      "\n",
      "“其实……没有。”罗恩苦笑一声，“但我们也没得选了。”\n",
      "\n",
      "赫敏点头，挥杖落下最后一道光线。\n",
      "\n",
      "整个魔法阵骤然亮起，泛出如水般的银光...\n",
      "\n",
      "Rank 5:\n",
      "  Distance: 0.477\n",
      "  Similarity: 0.677\n",
      "  Text: “还有我，”赫敏抬起头，“如果允许，我想继续研究裂缝魔法，也许这会是巫师历史新的开端。”\n",
      "\n",
      "邓布利多微微一笑：“霍格沃茨的图书馆，将永远为你敞开。”\n",
      "\n",
      "甘道夫从...\n"
     ]
    }
   ],
   "source": [
    "# Test with same query\n",
    "query = \"哈利波特用了什么魔法打败了索伦？\"\n",
    "results = retrieve(query, top_k=5, score_threshold=None)\n",
    "\n",
    "def print_top_k_result(results):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Results for \", query)\n",
    "    print(\"=\"*70)\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"  Distance: {r['distance']:.3f}\")  # Should now be 0.0-2.0 range\n",
    "        print(f\"  Similarity: {r['similarity']:.3f}\")\n",
    "        print(f\"  Text: {r['document'][:80]}...\")\n",
    "\n",
    "print_top_k_result(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f94c1-71a8-4a09-89fd-03c2e6de3ede",
   "metadata": {},
   "source": [
    "# Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eeb89c5-2ae6-4b23-9ac3-77e0a8fed660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ee3ea95d5944fb8ec8312cb0ee1fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification LOAD REPORT from: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Rerank: 2.668 | “准备好了？”赫敏看着他。\n",
      "\n",
      "“其实……没有。”罗恩苦笑一声，“但我们也没得选了。”\n",
      "\n",
      "赫敏点头，挥杖落下最后一道光线。\n",
      "\n",
      "整个魔法阵骤然亮起，泛出如水般的银光，直冲天际。火焰之眼被吸引，缓缓偏转方向，似乎被那魔戒的气息诱惑。\n",
      "甘道夫与邓布利多互看一眼，同时发动终极魔法。\n",
      "\n",
      "“Fianto Duri！”——钢铁守卫之盾。\n",
      "\n",
      "“Repello Inimicum！”——驱逐黑暗之敌。\n",
      "\n",
      "两道金色光柱自地面升起，交汇成一股炽烈光矛，直接击中火焰之眼侧翼。\n",
      "\n",
      "索伦发出痛苦咆哮，光明凤凰趁机直刺其中心。\n",
      "\n",
      "三股力量——哈利的光明守护神、两位巫师的联合光柱，以及甘道夫法杖中的太阳之火——汇聚成一道三色神光，灼穿天际，刺入那燃烧的瞳孔之中。\n",
      "\n",
      "黑暗开始崩塌。\n",
      "\n",
      "火焰退却，夜色翻滚。\n",
      "\n",
      "但索伦，尚未消亡。\n",
      "\n",
      "他正凝聚最后的黑暗力量，准备发动反扑——一击，足以吞噬所有。\n",
      "\n",
      "而这一刻，佛罗多缓缓举起至尊魔戒，做出了那个他一直不愿做出的选择……\n",
      "\n",
      "[1] Rerank: 0.816 | # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓气息，偶尔传来远处夜行魔兽的低吟。十六岁的哈利·波特紧握着魔杖，悄然穿行在林中。他的任务是寻找一只独角兽的毛发，作为奇兽饲育课的实地作业。\n",
      "\n",
      "“别走太远，”海格临走前叮嘱他，“那边有些地方连我都不熟。”\n",
      "\n",
      "突然，一道耀眼的金色光芒从一棵古老橡树后爆发出来，仿佛空间本身被撕裂。哈利猛然转身，举起魔杖高声念道：“荧光闪烁！”（Lumos）\n",
      "\n",
      "光芒之中，一个圆形漩涡缓缓旋转，宛如银河坠落凡间。等那道光慢慢散去，哈利惊讶地看到：面前出现了五个陌生人。\n",
      "\n",
      "一位身穿灰袍、头戴宽边帽、留着长须的高大老人静静站着，手持一根雕刻繁复的木杖。他身旁，是四个个子矮小但神情坚定的小个子生物，他们的脚上长满毛发，身着粗布外套，神情警觉。\n",
      "\n",
      "“你是谁？”哈利小心翼翼地问道。\n",
      "\n",
      "那老人微微一笑，语气温和：“我是甘道夫，一名中土世界的灰袍巫师。这四位是霍比特人——佛罗多、山姆、皮平和梅里。我们本应追踪一位古老的黑暗势力——索伦，却意外被卷入了这个世界。”\n",
      "\n",
      "哈利睁大眼睛，脑中一片混乱。他本以为霍格沃茨已经是个充满奇迹的地方，但眼前这些人的出现，却带来了更为深邃的谜团。他隐约感觉，这不仅是一场巧合，而是一场两个世界的交汇——一次足以改变命运的会面。\n",
      "\n",
      "[2] Rerank: -0.762 | 火焰之眼骤然收缩、崩塌，在空中发出最后一声爆响后，化作万千光点飘散。半兽人军团仿佛失去灵魂的傀儡，纷纷倒地，化为虚无。\n",
      "\n",
      "天边第一道金光破晓。\n",
      "\n",
      "哈利缓缓放下魔杖，脸色苍白，却眼神明亮。\n",
      "\n",
      "“结束了吗？”罗恩问。\n",
      "\n",
      "“……还没有。”甘道夫望着空中最后残留的光灰，“但我们赢得了喘息的机会。”\n",
      "\n",
      "凤凰在晨曦中盘旋，发出一声清越的鸣叫，仿佛宣告——黑夜过去，黎明已至。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(\n",
    "    query: str, \n",
    "    retrieved_results: List[dict], \n",
    "    top_k: int\n",
    ") -> List[dict]:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    \n",
    "    # Extract documents from retrieve() results\n",
    "    chunks = [result['document'] for result in retrieved_results]\n",
    "    pairs = [(query, chunk) for chunk in chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Preserve full result dict with rerank score\n",
    "    for result, score in zip(retrieved_results, scores):\n",
    "        result['rerank_score'] = float(score)\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    retrieved_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    return retrieved_results[:top_k]\n",
    "\n",
    "# Usage\n",
    "retrieved = retrieve(query, top_k=5)\n",
    "reranked = rerank(query, retrieved, top_k=3)\n",
    "\n",
    "for i, result in enumerate(reranked):\n",
    "    print(f\"[{i}] Rerank: {result['rerank_score']:.3f} | {result['document']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74328fe1-e65d-4e82-85bf-7db31d21a404",
   "metadata": {},
   "source": [
    "# LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063a02c-ea6a-4bb0-9943-959500523538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
