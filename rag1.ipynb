{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4523d453-48f7-4dc7-a687-987cb9d8ce7c",
   "metadata": {},
   "source": [
    "# Raw Document Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431be288-3c0a-4498-8bc5-4461fe9efe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 8085\n",
      "Chinese characters: 6433 (≈ words)\n",
      "English words: 24\n",
      "Approximate total words: 6457\n"
     ]
    }
   ],
   "source": [
    "def word_count(file_name: str) -> None:\n",
    "    # More accurate word count for Chinese\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count Chinese characters (each Chinese char ≈ 1 word)\n",
    "    chinese_chars = len([c for c in content if '\\u4e00' <= c <= '\\u9fff'])\n",
    "    # Count English words\n",
    "    english_words = len([w for w in content.split() if any('a' <= c.lower() <= 'z' for c in w)])\n",
    "    \n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"Chinese characters: {chinese_chars} (≈ words)\")\n",
    "    print(f\"English words: {english_words}\")\n",
    "    print(f\"Approximate total words: {chinese_chars + english_words}\")\n",
    "\n",
    "word_count(\"cn1.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516fda4-83b1-4866-8ed8-f7dac6fbe80f",
   "metadata": {},
   "source": [
    "# Splitting and Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fac282-e1c2-4a8d-bd5f-ebd704c2811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_into_chunks(\n",
    "    doc_file: str, \n",
    "    chunk_size: int = 800, \n",
    "    chunk_overlap: int = 150\n",
    ") -> List[str]:\n",
    "    with open(doc_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \"\\n\", \"。\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(content)\n",
    "\n",
    "chunks = split_into_chunks(\"cn1.md\")\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc280e-38d5-4c9f-bb8f-e1bf35dc850b",
   "metadata": {},
   "source": [
    "# Indexing and Storage\n",
    "Embedding Model Choice:\n",
    "- `shibing624/text2vec-base-chinese` (good for Chinese and used in `rag0`)\n",
    "- `BAAI/bge-base-zh-v1.5` (better Chinese performance)\n",
    "- `moka-ai/m3e-base` (multilingual Chinese-English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a827e8-b9f9-4d4e-a167-862450fa0720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbba52743fb493195bce5d3c1d337ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted old collection\n",
      "Created collection with metadata: {'hnsw:space': 'cosine'}\n",
      "Generated 15 embeddings\n",
      "Embedding dimension: 768\n",
      "Saved 15 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "# ======================================== \n",
    "# Initialize models\n",
    "# ========================================\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Create ChromaDB with correct settings\n",
    "# ========================================\n",
    "def create_db():\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "    # If you want to GUARANTEE cosine space, delete+recreate:\n",
    "    try:\n",
    "        client.delete_collection(name=\"default\")\n",
    "        print(\"Deleted old collection\")\n",
    "    except Exception as e:\n",
    "        # ok if it doesn't exist; still good to know unexpected errors\n",
    "        print(f\"(delete_collection) {e}\")\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=\"default\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    print(f\"Created collection with metadata: {collection.metadata}\")\n",
    "    return collection\n",
    "\n",
    "chromadb_collection = create_db()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Embed\n",
    "# ========================================\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    emb = embedding_model.encode(chunk)  # numpy array\n",
    "    return emb.tolist()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Store with metadata\n",
    "# ========================================\n",
    "def save_embeddings(\n",
    "    collection,\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    "    source_file: str,\n",
    ") -> None:\n",
    "    if not chunks:\n",
    "        raise ValueError(\"chunks is empty\")\n",
    "\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(f\"chunks ({len(chunks)}) and embeddings ({len(embeddings)}) length mismatch\")\n",
    "    \n",
    "    if not source_file:\n",
    "        return ValueError(\"file is empty\")\n",
    "\n",
    "    # Use stable IDs that won't collide across different files\n",
    "    ids = [f\"{source_file}:{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": source_file,\n",
    "            \"chunk_length\": len(chunk),\n",
    "            \"chunk_index\": i,\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Use upsert so reruns don't explode on duplicate ids\n",
    "    collection.upsert(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# embeddings\n",
    "embeddings = [embed_chunk(c) for c in chunks]\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "save_embeddings(chromadb_collection, chunks, embeddings, \"cn1.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae254c19-7af1-4f4b-9824-6cfcc7b9119a",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bfcee1-6e7d-4ce0-a351-ddf24acdff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5, score_threshold = None):\n",
    "    query_embedding = embed_chunk(query)\n",
    "    \n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"No results found!\")\n",
    "        return []\n",
    "    \n",
    "    retrieved = []\n",
    "    for i, (doc, dist, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    )):\n",
    "        if score_threshold is None or dist <= score_threshold:\n",
    "            retrieved.append({\n",
    "                'document': doc,\n",
    "                'distance': dist,\n",
    "                'similarity': 1 / (1 + dist),\n",
    "                'metadata': meta,\n",
    "                'rank': i\n",
    "            })\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved)}/{top_k} chunks\")\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36de35d-a389-4548-83ed-959db76c7bfd",
   "metadata": {},
   "source": [
    "## Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640899dd-c31f-4afd-be9a-d63b7e86c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n",
      "\n",
      "======================================================================\n",
      "Results for  哈利的守护神是什么样子的？\n",
      "======================================================================\n",
      "\n",
      "Rank 1:\n",
      "  Distance: 0.454\n",
      "  Similarity: 0.688\n",
      "  Text: ## 第二章：邓布利多的召见\n",
      "\n",
      "哈利意识到自己必须马上通知学校。他立刻从口袋中掏出一枚银色硬币般的护符，深吸一口气，默念咒语：“Expecto Patronum...\n",
      "\n",
      "Rank 2:\n",
      "  Distance: 0.455\n",
      "  Similarity: 0.687\n",
      "  Text: # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓...\n",
      "\n",
      "Rank 3:\n",
      "  Distance: 0.465\n",
      "  Similarity: 0.683\n",
      "  Text: ## 第五章：魔法的融合\n",
      "\n",
      "临战前夜，霍格沃茨的钟楼熄灯，取而代之的是城堡上空缓缓旋转的守护星光魔阵。夜色中笼罩着一股难以名状的寂静，仿佛整个世界都屏息等待。\n",
      "...\n",
      "\n",
      "Rank 4:\n",
      "  Distance: 0.482\n",
      "  Similarity: 0.675\n",
      "  Text: “我们可以反向布阵，让索伦误以为魔戒就在阵心，引诱他完全现身。”赫敏解释道，“只要他现身，我们的咒语就能发挥最大效力。”\n",
      "\n",
      "佛罗多脸色惨白：“但那意味着我必须站...\n",
      "\n",
      "Rank 5:\n",
      "  Distance: 0.516\n",
      "  Similarity: 0.659\n",
      "  Text: “还有我，”赫敏抬起头，“如果允许，我想继续研究裂缝魔法，也许这会是巫师历史新的开端。”\n",
      "\n",
      "邓布利多微微一笑：“霍格沃茨的图书馆，将永远为你敞开。”\n",
      "\n",
      "甘道夫从...\n"
     ]
    }
   ],
   "source": [
    "# query = \"哈利波特用了什么魔法打败了索伦？\"\n",
    "query = \"哈利的守护神是什么样子的？\"\n",
    "results = retrieve(query, top_k=5, score_threshold=None)\n",
    "\n",
    "def print_top_k_result(results):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Results for \", query)\n",
    "    print(\"=\"*70)\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"  Distance: {r['distance']:.3f}\")  # Should now be 0.0-2.0 range\n",
    "        print(f\"  Similarity: {r['similarity']:.3f}\")\n",
    "        print(f\"  Text: {r['document'][:80]}...\")\n",
    "\n",
    "print_top_k_result(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f94c1-71a8-4a09-89fd-03c2e6de3ede",
   "metadata": {},
   "source": [
    "# Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eeb89c5-2ae6-4b23-9ac3-77e0a8fed660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(\n",
    "    query: str, \n",
    "    retrieved_results: List[dict], \n",
    "    top_k: int\n",
    ") -> List[dict]:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    \n",
    "    # Extract documents from retrieve() results\n",
    "    chunks = [result['document'] for result in retrieved_results]\n",
    "    pairs = [(query, chunk) for chunk in chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Preserve full result dict with rerank score\n",
    "    for result, score in zip(retrieved_results, scores):\n",
    "        result['rerank_score'] = float(score)\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    retrieved_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    return retrieved_results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5addfd-e1bb-4870-9ff2-cad93742fec8",
   "metadata": {},
   "source": [
    "## Rerank Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adf4adc-5298-452a-90fc-b70c59519d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585fe2ed60e64579a746408e5379d299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification LOAD REPORT from: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Rerank: 2.337 | ## 第二章：邓布利多的召见\n",
      "\n",
      "哈利意识到自己必须马上通知学校。他立刻从口袋中掏出一枚银色硬币般的护符，深吸一口气，默念咒语：“Expecto Patronum\n",
      "\n",
      "[1] Rerank: 1.561 | ## 第五章：魔法的融合\n",
      "\n",
      "临战前夜，霍格沃茨的钟楼熄灯，取而代之的是城堡上空缓缓旋转的守护星光魔阵。夜色中笼罩着一股难以名状的寂静，仿佛整个世界都屏息等待。\n",
      "\n",
      "\n",
      "[2] Rerank: -0.051 | # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieved = retrieve(query, top_k=5)\n",
    "reranked = rerank(query, retrieved, top_k=3)\n",
    "\n",
    "for i, result in enumerate(reranked):\n",
    "    print(f\"[{i}] Rerank: {result['rerank_score']:.3f} | {result['document'][:80]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74328fe1-e65d-4e82-85bf-7db31d21a404",
   "metadata": {},
   "source": [
    "# LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c063a02c-ea6a-4bb0-9943-959500523538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from typing import List\n",
    "\n",
    "load_dotenv()\n",
    "google_client = genai.Client()\n",
    "\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    # Combine retrieved chunks into context\n",
    "    context = \"\\n\\n\".join([f\"[片段 {i+1}]\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
    "    \n",
    "    prompt = f\"\"\"你是一个专业的小说问答助手。请基于以下提供的小说片段来回答用户的问题。\n",
    "\n",
    "小说片段:\n",
    "{context}\n",
    "\n",
    "用户问题: {query}\n",
    "\n",
    "请根据上述片段提供准确、详细的回答。不要编造信息。如果片段中没有足够信息回答问题,请说明。\"\"\"\n",
    "\n",
    "    response = google_client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127143f-8528-4e5e-bd1b-7a5631587a1f",
   "metadata": {},
   "source": [
    "## Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0d67a8-22c7-4aac-b3c0-d40650e2143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4b5ec82a6c46cba6fb6233d6f4789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification LOAD REPORT from: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据小说片段的描述，哈利的守护神在不同阶段有不同的形态：\n",
      "\n",
      "1.  **初始形态（片段1）**：哈利的守护神是**一头牡鹿状的**，伴随一道银光腾空而起。\n",
      "2.  **融合后的形态（片段2）**：在甘道夫的指引下，并吸收了中土世界最纯净的圣光——阿尔诺之焰后，哈利的守护神发生了变化。它变成了一道**巨大的银色凤凰**，展开羽翼，羽毛上镶嵌着**星光般的符文**，胸口蕴藏着**甘道夫圣光的核心**。这段描述也指出，它既是哈利的守护神，也是光明本身。\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "query = \"哈利的守护神是什么样子的？\"\n",
    "retrieved = retrieve(query, top_k=5)\n",
    "reranked = rerank(query, retrieved, top_k=3)\n",
    "reranked_chunks = [result['document'] for result in reranked]\n",
    "\n",
    "answer = generate(query, reranked_chunks)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
