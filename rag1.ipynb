{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4523d453-48f7-4dc7-a687-987cb9d8ce7c",
   "metadata": {},
   "source": [
    "# Raw Document Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431be288-3c0a-4498-8bc5-4461fe9efe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1897\n",
      "Chinese characters: 1483 (≈ words)\n",
      "English words: 19\n",
      "Approximate total words: 1502\n",
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "def word_count(file_name: str) -> None:\n",
    "    # More accurate word count for Chinese\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count Chinese characters (each Chinese char ≈ 1 word)\n",
    "    chinese_chars = len([c for c in content if '\\u4e00' <= c <= '\\u9fff'])\n",
    "    # Count English words\n",
    "    english_words = len([w for w in content.split() if any('a' <= c.lower() <= 'z' for c in w)])\n",
    "    \n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"Chinese characters: {chinese_chars} (≈ words)\")\n",
    "    print(f\"English words: {english_words}\")\n",
    "    print(f\"Approximate total words: {chinese_chars + english_words}\")\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "word_count(\"cn.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9e1bc-b4bd-48b1-81bb-434cfe28a29c",
   "metadata": {},
   "source": [
    ">**5 chunks is actually good enough for a short story (i.e., 1,897 chars).** (i) Fast retrieval speed as only 5 embeddings to search through; And (ii) context preservation since each chunk has ~400 chars - enough context for a complete scene.\n",
    ">\n",
    ">**You'd want 10-20+ chunks if** (i) Document is 10,000+ characters (long article/book); (ii) Very specific factual queries requiring granular retrieval; (iii) Multiple topics in one document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516fda4-83b1-4866-8ed8-f7dac6fbe80f",
   "metadata": {},
   "source": [
    "# Splitting and Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fac282-e1c2-4a8d-bd5f-ebd704c2811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_into_chunks(doc_file: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:\n",
    "    with open(doc_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(content)\n",
    "\n",
    "chunks = split_into_chunks(\"cn.md\")\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n",
    "\n",
    "# for i, c in enumerate(chunks):\n",
    "#     print(f\"[{i}] {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc280e-38d5-4c9f-bb8f-e1bf35dc850b",
   "metadata": {},
   "source": [
    "# Indexing and Storage\n",
    "Embedding Model Choice:\n",
    "- `shibing624/text2vec-base-chinese` (good for Chinese and used in `rag0`)\n",
    "- `BAAI/bge-base-zh-v1.5` (better Chinese performance)\n",
    "- `moka-ai/m3e-base` (multilingual Chinese-English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a827e8-b9f9-4d4e-a167-862450fa0720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ba0b3e052c41c698343c8d5d280293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(delete_collection) Collection [default] does not exist\n",
      "Created collection with metadata: {'hnsw:space': 'cosine'}\n",
      "Generated 5 embeddings\n",
      "Embedding dimension: 768\n",
      "Saved 5 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "# ======================================== \n",
    "# Initialize models\n",
    "# ========================================\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Create ChromaDB with correct settings\n",
    "# ========================================\n",
    "def create_db():\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "    # If you want to GUARANTEE cosine space, delete+recreate:\n",
    "    try:\n",
    "        client.delete_collection(name=\"default\")\n",
    "        print(\"Deleted old collection\")\n",
    "    except Exception as e:\n",
    "        # ok if it doesn't exist; still good to know unexpected errors\n",
    "        print(f\"(delete_collection) {e}\")\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=\"default\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    print(f\"Created collection with metadata: {collection.metadata}\")\n",
    "    return collection\n",
    "\n",
    "chromadb_collection = create_db()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Embed\n",
    "# ========================================\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    emb = embedding_model.encode(chunk)  # numpy array\n",
    "    return emb.tolist()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Store with metadata\n",
    "# ========================================\n",
    "def save_embeddings(\n",
    "    collection,\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    "    source_file: str = \"cn.md\",\n",
    ") -> None:\n",
    "    if not chunks:\n",
    "        raise ValueError(\"chunks is empty\")\n",
    "\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(f\"chunks ({len(chunks)}) and embeddings ({len(embeddings)}) length mismatch\")\n",
    "\n",
    "    # Use stable IDs that won't collide across different files\n",
    "    ids = [f\"{source_file}:{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": source_file,\n",
    "            \"chunk_length\": len(chunk),\n",
    "            \"chunk_index\": i,\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Use upsert so reruns don't explode on duplicate ids\n",
    "    collection.upsert(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# embeddings\n",
    "embeddings = [embed_chunk(c) for c in chunks]\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "save_embeddings(chromadb_collection, chunks, embeddings, \"cn.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae254c19-7af1-4f4b-9824-6cfcc7b9119a",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bfcee1-6e7d-4ce0-a351-ddf24acdff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5, score_threshold = None):\n",
    "    query_embedding = embed_chunk(query)\n",
    "    \n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"No results found!\")\n",
    "        return []\n",
    "    \n",
    "    retrieved = []\n",
    "    for i, (doc, dist, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    )):\n",
    "        if score_threshold is None or dist <= score_threshold:\n",
    "            retrieved.append({\n",
    "                'document': doc,\n",
    "                'distance': dist,\n",
    "                'similarity': 1 / (1 + dist),\n",
    "                'metadata': meta,\n",
    "                'rank': i\n",
    "            })\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved)}/{top_k} chunks\")\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36de35d-a389-4548-83ed-959db76c7bfd",
   "metadata": {},
   "source": [
    "## Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "640899dd-c31f-4afd-be9a-d63b7e86c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3/3 chunks\n",
      "\n",
      "======================================================================\n",
      "Results for  哈利波特用了什么魔法打败了索伦？\n",
      "======================================================================\n",
      "\n",
      "Rank 1:\n",
      "  Distance: 0.358\n",
      "  Similarity: 0.736\n",
      "  Text: ## 第八章：最后一击\n",
      "\n",
      "索伦发出震耳欲聋的咆哮，但还没有被完全击败。他将所有的黑暗力量集中到一点，准备做最后的反击。就在这个关键时刻，佛罗多举起至尊魔戒，大喊...\n",
      "\n",
      "Rank 2:\n",
      "  Distance: 0.370\n",
      "  Similarity: 0.730\n",
      "  Text: ## 第五章：魔法的融合\n",
      "\n",
      "在甘道夫的指导下，哈利学会了如何将他的守护神咒与中土世界的光明魔法结合。他们制定了一个大胆的计划：哈利将使用\"Expecto Pat...\n",
      "\n",
      "Rank 3:\n",
      "  Distance: 0.416\n",
      "  Similarity: 0.706\n",
      "  Text: # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林深处，哈利·波特正在寻找独角兽的踪迹，完成海格布置的神奇生物课作业。突然，一道耀眼...\n"
     ]
    }
   ],
   "source": [
    "# Test with same query\n",
    "query = \"哈利波特用了什么魔法打败了索伦？\"\n",
    "results = retrieve(query, top_k=3, score_threshold=None)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Results for \", query)\n",
    "print(\"=\"*70)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\nRank {i+1}:\")\n",
    "    print(f\"  Distance: {r['distance']:.3f}\")  # Should now be 0.0-2.0 range\n",
    "    print(f\"  Similarity: {r['similarity']:.3f}\")\n",
    "    print(f\"  Text: {r['document'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f94c1-71a8-4a09-89fd-03c2e6de3ede",
   "metadata": {},
   "source": [
    "# Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb89c5-2ae6-4b23-9ac3-77e0a8fed660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:\n",
    "    corss_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = corss_encoder.predict(pairs)\n",
    "\n",
    "    scored_chunks = [(chunk, score) for chunk, score in zip(retrieved_chunks, scores)]\n",
    "    scored_chunks.sort(key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "    # we only return text chunk but get rid of score\n",
    "    return [chunk for chunk, _ in scored_chunks][:top_k]\n",
    "\n",
    "reranked_chunks = rerank(query, retrieved_chunks, 3)\n",
    "\n",
    "# Print top-K result out\n",
    "for i, chunk in enumerate(reranked_chunks):\n",
    "    print(f\"[{i}] {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74328fe1-e65d-4e82-85bf-7db31d21a404",
   "metadata": {},
   "source": [
    "# LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063a02c-ea6a-4bb0-9943-959500523538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
