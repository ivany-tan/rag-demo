{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1aff14-0d44-4447-9790-98bd29ea5551",
   "metadata": {},
   "source": [
    "# **Build RAG from Scratch**\n",
    "\n",
    "- Step 1: Document Loading and Preprocessing\n",
    "- Step 2: Text Splitting and Chunking\n",
    "- Step 3: Embedding Generation (Indexing)\n",
    "- Step 4: Vector Database Storage\n",
    "- Step 5: Query Processing and Retrieval: Perform similarity search (Dense Retrieval) and retrieve top-K candidates\n",
    "- Step 6: Reranking: Initialize cross-encoder model, rerank retrieved candidates and filter to top-N results\n",
    "- Step 7: Context Preparation for LLM: Format retrieved chunks\n",
    "- Step 8: LLM Generation (Optional): Initialize LLM and generate response with context\n",
    "- _Evaluation and Testing_: Test queries, Evaluation metrics and Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523d453-48f7-4dc7-a687-987cb9d8ce7c",
   "metadata": {},
   "source": [
    "# Raw Document Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431be288-3c0a-4498-8bc5-4461fe9efe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 8085\n",
      "Chinese characters: 6433 (≈ words)\n",
      "English words: 24\n",
      "Approximate total words: 6457\n"
     ]
    }
   ],
   "source": [
    "def word_count(file_name: str) -> None:\n",
    "    # More accurate word count for Chinese\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count Chinese characters (each Chinese char ≈ 1 word)\n",
    "    chinese_chars = len([c for c in content if '\\u4e00' <= c <= '\\u9fff'])\n",
    "    # Count English words\n",
    "    english_words = len([w for w in content.split() if any('a' <= c.lower() <= 'z' for c in w)])\n",
    "    \n",
    "    print(f\"Total characters: {len(content)}\")\n",
    "    print(f\"Chinese characters: {chinese_chars} (≈ words)\")\n",
    "    print(f\"English words: {english_words}\")\n",
    "    print(f\"Approximate total words: {chinese_chars + english_words}\")\n",
    "\n",
    "word_count(\"cn1.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516fda4-83b1-4866-8ed8-f7dac6fbe80f",
   "metadata": {},
   "source": [
    "# Part 1: Splitting and Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fac282-e1c2-4a8d-bd5f-ebd704c2811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_into_chunks(\n",
    "    doc_file: str, \n",
    "    chunk_size: int = 800, \n",
    "    chunk_overlap: int = 150\n",
    ") -> List[str]:\n",
    "    with open(doc_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n## \", \"\\n# \", \"\\n\\n\", \"\\n\", \"。\", \"!\", \"?\", \";\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_text(content)\n",
    "\n",
    "chunks = split_into_chunks(\"cn1.md\")\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc280e-38d5-4c9f-bb8f-e1bf35dc850b",
   "metadata": {},
   "source": [
    "# Part 2: Indexing and Storage\n",
    "**ChromaDB handles _cosine_ similarity behind the scenes** - that's the whole point of using a vector database instead of computing similarities manually. `client.create_collection(..., metadata={\"hnsw:space\": \"cosine\"})`\n",
    "\n",
    "Embedding Model Choice:\n",
    "- `shibing624/text2vec-base-chinese` (good for Chinese and used in `rag0`)\n",
    "- `BAAI/bge-base-zh-v1.5` (better Chinese performance)\n",
    "- `moka-ai/m3e-base` (multilingual Chinese-English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a827e8-b9f9-4d4e-a167-862450fa0720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c2f322d9044f34ab0c801800c04cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted old collection\n",
      "Created collection with metadata: {'hnsw:space': 'cosine'}\n",
      "Generated 15 embeddings\n",
      "Embedding dimension: 768\n",
      "Saved 15 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "# ======================================== \n",
    "# Initialize models\n",
    "# ========================================\n",
    "embedding_model = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Create ChromaDB with correct settings\n",
    "# ========================================\n",
    "def create_db():\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "    # If you want to GUARANTEE cosine space, delete+recreate:\n",
    "    try:\n",
    "        client.delete_collection(name=\"default\")\n",
    "        print(\"Deleted old collection\")\n",
    "    except Exception as e:\n",
    "        # ok if it doesn't exist; still good to know unexpected errors\n",
    "        print(f\"(delete_collection) {e}\")\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=\"default\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    print(f\"Created collection with metadata: {collection.metadata}\")\n",
    "    return collection\n",
    "\n",
    "chromadb_collection = create_db()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Embed\n",
    "# ========================================\n",
    "def embed_chunk(chunk: str) -> List[float]:\n",
    "    emb = embedding_model.encode(chunk)  # numpy array\n",
    "    return emb.tolist()\n",
    "\n",
    "\n",
    "# ======================================== \n",
    "# Store with metadata\n",
    "# ========================================\n",
    "def save_embeddings(\n",
    "    collection,\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    "    source_file: str,\n",
    ") -> None:\n",
    "    if not chunks:\n",
    "        raise ValueError(\"chunks is empty\")\n",
    "\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(f\"chunks ({len(chunks)}) and embeddings ({len(embeddings)}) length mismatch\")\n",
    "    \n",
    "    if not source_file:\n",
    "        return ValueError(\"file is empty\")\n",
    "\n",
    "    # Use stable IDs that won't collide across different files\n",
    "    ids = [f\"{source_file}:{i}\" for i in range(len(chunks))]\n",
    "\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": source_file,\n",
    "            \"chunk_length\": len(chunk),\n",
    "            \"chunk_index\": i,\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Use upsert so reruns don't explode on duplicate ids\n",
    "    collection.upsert(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {len(chunks)} chunks to ChromaDB\")\n",
    "\n",
    "# embeddings\n",
    "embeddings = [embed_chunk(c) for c in chunks]\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "save_embeddings(chromadb_collection, chunks, embeddings, \"cn1.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae254c19-7af1-4f4b-9824-6cfcc7b9119a",
   "metadata": {},
   "source": [
    "# Part 3: Retrieval\n",
    "\n",
    "Step 1 (in part 3): **Bi-encoder** Retrieval (Fast but less accurate)\n",
    "- Encode query and docs separately → Compare vectors with cosine similarity\n",
    "- Get top-K candidates i.e., 5, 10 or 20\n",
    "- Time: ~0.03s\n",
    "\n",
    "Step 2 (in part 4): **Cross-encoder** Reranking (Slow but very accurate)  \n",
    "- Process (query + doc) together with attention → Get relevance score\n",
    "- Rerank top-K → top 3 results\n",
    "- Time: ~2.5s\n",
    "\n",
    ">**Q**: Why not use cross-encoder only?\n",
    ">\n",
    ">**A**: Too expensive! Cross-encoder must process ALL documents (10K docs = \n",
    "   500s per query). Bi-encoder caches document embeddings, so only needs \n",
    "   to encode query (0.03s). Result: 167× speedup.\n",
    "\n",
    "Key Insight: Bi-encoder casts wide net efficiently, cross-encoder filters to highest quality. But if retrieval can return high quality results, the rerank is not a must."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bfcee1-6e7d-4ce0-a351-ddf24acdff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k: int = 5, score_threshold = None):\n",
    "    query_embedding = embed_chunk(query)\n",
    "    \n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        print(\"No results found!\")\n",
    "        return []\n",
    "    \n",
    "    retrieved = []\n",
    "    for i, (doc, dist, meta) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['distances'][0],\n",
    "        results['metadatas'][0]\n",
    "    )):\n",
    "        if score_threshold is None or dist <= score_threshold:\n",
    "            retrieved.append({\n",
    "                'document': doc,\n",
    "                'distance': dist,  # This is actually cosine distance (1 - cosine_similarity)\n",
    "                'similarity': 1 / (1 + dist), # Manual conversion to similarity score from 0 to 1 - more intuitive to read\n",
    "                'metadata': meta,\n",
    "                'rank': i\n",
    "            })\n",
    "    \n",
    "    print(f\"Retrieved {len(retrieved)}/{top_k} chunks\")\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36de35d-a389-4548-83ed-959db76c7bfd",
   "metadata": {},
   "source": [
    "## Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640899dd-c31f-4afd-be9a-d63b7e86c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n",
      "\n",
      "======================================================================\n",
      "Results for  哈利用什么魔法打败了索伦？\n",
      "======================================================================\n",
      "\n",
      "Rank 1:\n",
      "    Distance: 0.432\n",
      "    Similarity: 0.698\n",
      "    Text: ## 第五章：魔法的融合\n",
      "\n",
      "临战前夜，霍格沃茨的钟楼熄灯，取而代之的是城堡上空缓缓旋转的守护星光魔阵。夜色中笼罩着一股难以名状的寂静，仿佛整个世界都屏息等待。\n",
      "...\n",
      "\n",
      "Rank 2:\n",
      "    Distance: 0.439\n",
      "    Similarity: 0.695\n",
      "    Text: # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓...\n",
      "\n",
      "Rank 3:\n",
      "    Distance: 0.471\n",
      "    Similarity: 0.680\n",
      "    Text: 火焰之眼骤然收缩、崩塌，在空中发出最后一声爆响后，化作万千光点飘散。半兽人军团仿佛失去灵魂的傀儡，纷纷倒地，化为虚无。\n",
      "\n",
      "天边第一道金光破晓。\n",
      "\n",
      "哈利缓缓放下魔...\n",
      "\n",
      "Rank 4:\n",
      "    Distance: 0.480\n",
      "    Similarity: 0.675\n",
      "    Text: “还有我，”赫敏抬起头，“如果允许，我想继续研究裂缝魔法，也许这会是巫师历史新的开端。”\n",
      "\n",
      "邓布利多微微一笑：“霍格沃茨的图书馆，将永远为你敞开。”\n",
      "\n",
      "甘道夫从...\n",
      "\n",
      "Rank 5:\n",
      "    Distance: 0.483\n",
      "    Similarity: 0.674\n",
      "    Text: “准备好了？”赫敏看着他。\n",
      "\n",
      "“其实……没有。”罗恩苦笑一声，“但我们也没得选了。”\n",
      "\n",
      "赫敏点头，挥杖落下最后一道光线。\n",
      "\n",
      "整个魔法阵骤然亮起，泛出如水般的银光...\n"
     ]
    }
   ],
   "source": [
    "# query = \"哈利波特用了什么魔法打败了索伦？\"\n",
    "query = \"哈利用什么魔法打败了索伦？\"\n",
    "results = retrieve(query, top_k=5, score_threshold=None)\n",
    "\n",
    "def print_top_k_result(results):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Results for \", query)\n",
    "    print(\"=\"*70)\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"    Distance: {r['distance']:.3f}\")  # Should now be 0.0-2.0 range\n",
    "        print(f\"    Similarity: {r['similarity']:.3f}\")\n",
    "        print(f\"    Text: {r['document'][:80]}...\")\n",
    "\n",
    "print_top_k_result(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f94c1-71a8-4a09-89fd-03c2e6de3ede",
   "metadata": {},
   "source": [
    "# Part 4: Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eeb89c5-2ae6-4b23-9ac3-77e0a8fed660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(\n",
    "    query: str, \n",
    "    retrieved_results: List[dict], \n",
    "    top_k: int\n",
    ") -> List[dict]:\n",
    "    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    \n",
    "    # Extract documents from retrieve() results\n",
    "    chunks = [result['document'] for result in retrieved_results]\n",
    "    pairs = [(query, chunk) for chunk in chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Preserve full result dict with rerank score\n",
    "    for result, score in zip(retrieved_results, scores):\n",
    "        result['rerank_score'] = float(score)\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    retrieved_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    return retrieved_results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5addfd-e1bb-4870-9ff2-cad93742fec8",
   "metadata": {},
   "source": [
    "## Rerank Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adf4adc-5298-452a-90fc-b70c59519d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e969a26076d446c8b128b616aa279ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification LOAD REPORT from: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Rerank: 3.363 | “准备好了？”赫敏看着他。\n",
      "\n",
      "“其实……没有。”罗恩苦笑一声，“但我们也没得选了。”\n",
      "\n",
      "赫敏点头，挥杖落下最后一道光线。\n",
      "\n",
      "整个魔法阵骤然亮起，泛出如水般的银光\n",
      "\n",
      "[1] Rerank: -0.106 | # 魔戒与魔杖：两个世界的交汇\n",
      "\n",
      "## 第一章：神秘的传送门\n",
      "\n",
      "霍格沃茨的禁林，夜色正浓，月光从浓密树冠的缝隙中洒落，投下斑驳的银色光影。空气中弥漫着湿润的苔藓\n",
      "\n",
      "[2] Rerank: -0.479 | ## 第五章：魔法的融合\n",
      "\n",
      "临战前夜，霍格沃茨的钟楼熄灯，取而代之的是城堡上空缓缓旋转的守护星光魔阵。夜色中笼罩着一股难以名状的寂静，仿佛整个世界都屏息等待。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieved = retrieve(query, top_k=5)\n",
    "reranked = rerank(query, retrieved, top_k=3)\n",
    "\n",
    "for i, result in enumerate(reranked):\n",
    "    print(f\"[{i}] Rerank: {result['rerank_score']:.3f} | {result['document'][:80]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74328fe1-e65d-4e82-85bf-7db31d21a404",
   "metadata": {},
   "source": [
    "# Part 5: LLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c063a02c-ea6a-4bb0-9943-959500523538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from typing import List\n",
    "\n",
    "load_dotenv()\n",
    "google_client = genai.Client()\n",
    "\n",
    "def generate(query: str, chunks: List[str]) -> str:\n",
    "    # Combine retrieved chunks into context\n",
    "    context = \"\\n\\n\".join([f\"[片段 {i+1}]\\n{chunk}\" for i, chunk in enumerate(chunks)])\n",
    "    \n",
    "    prompt = f\"\"\"你是一个专业的小说问答助手。请基于以下提供的小说片段来回答用户的问题。\n",
    "\n",
    "小说片段:\n",
    "{context}\n",
    "\n",
    "用户问题: {query}\n",
    "\n",
    "请根据上述片段提供准确、详细的回答。不要编造信息。如果片段中没有足够信息回答问题,请说明。\"\"\"\n",
    "\n",
    "    response = google_client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127143f-8528-4e5e-bd1b-7a5631587a1f",
   "metadata": {},
   "source": [
    "## LLM Generation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0d67a8-22c7-4aac-b3c0-d40650e2143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5/5 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61634eefa5f24398bc94c1885ef8d59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification LOAD REPORT from: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据小说片段提供的信息，哈利利用他的**光明守护神**（巨大的银色凤凰形态）来打败索伦。\n",
      "\n",
      "这个光明守护神并非普通的守护神咒，它是在甘道夫的帮助下得到强化的：\n",
      "\n",
      "1.  甘道夫将“阿尔诺之焰”（中土最纯净的圣光）汇入哈利的手心，并让其融入哈利的魔杖。\n",
      "2.  哈利通过集中精神，回想起爱和希望（父母、西里斯、邓布利多、朋友），最终成功召唤出“一道巨大的银色凤凰”。\n",
      "3.  这只凤凰的羽毛上镶嵌着星光般的符文，胸口蕴藏着甘道夫圣光的核心，它既是哈利的守护神，也是光明本身，代表了“两个世界希望的共鸣”。\n",
      "\n",
      "在最终的战斗中，这股力量与**两位巫师（甘道夫和邓布利多）的联合光柱**以及**甘道夫法杖中的太阳之火**汇聚成一道三色神光，共同灼穿了索伦燃烧的瞳孔，使得黑暗开始崩塌。\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "query = \"哈利用什么魔法打败了索伦？\"\n",
    "retrieved = retrieve(query, top_k=5)\n",
    "reranked = rerank(query, retrieved, top_k=3)\n",
    "reranked_chunks = [result['document'] for result in reranked]\n",
    "\n",
    "answer = generate(query, reranked_chunks)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0c91a-3c84-483b-a93d-5a55cbd5b03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
