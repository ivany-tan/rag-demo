{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fa13bc-66d1-4c36-b060-b024039907d7",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3fe6d9-d76d-4d40-804b-54ab0994be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain core\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredFileLoader,\n",
    "    DirectoryLoader,\n",
    "    WebBaseLoader,\n",
    ")\n",
    "\n",
    "# Vector store and embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger('IndustrialRAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ff3aa-25e6-4685-9ed8-c88cae31b8a2",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d5d032-d40a-417c-8ee1-4ca91cdc69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses tiktoken (OpenAI's tokenizer) as a proxy for token estimation\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        tokens = encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error counting tokens: {e}\")\n",
    "        # Fallback: rough estimation (1 token â‰ˆ 4 characters)\n",
    "        return len(text) // 4\n",
    "\n",
    "def analyze_document(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"   \n",
    "    Strategy:\n",
    "    - Short docs (<2000 tokens): chunk_size=500, overlap=100\n",
    "    - Medium docs (2000-10000 tokens): chunk_size=1000, overlap=200\n",
    "    - Long docs (>10000 tokens): chunk_size=1500, overlap=300\n",
    "    Returns: Dictionary with token count and suggested chunk parameters\n",
    "    \"\"\"\n",
    "    token_count = count_tokens(text)\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    \n",
    "    # Determine optimal chunk size based on document length\n",
    "    if token_count < 2000:\n",
    "        chunk_size = 500\n",
    "        chunk_overlap = 100\n",
    "        strategy = \"small\"\n",
    "    elif token_count < 10000:\n",
    "        chunk_size = 1000\n",
    "        chunk_overlap = 200\n",
    "        strategy = \"medium\"\n",
    "    else:\n",
    "        chunk_size = 1500\n",
    "        chunk_overlap = 300\n",
    "        strategy = \"large\"\n",
    "    \n",
    "    analysis = {\n",
    "        \"token_count\": token_count,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"suggested_chunk_size\": chunk_size,\n",
    "        \"suggested_overlap\": chunk_overlap,\n",
    "        \"strategy\": strategy\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Document analysis: {token_count} tokens, strategy={strategy}\")\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9a4b7-9168-40a5-99f1-5890ac60de86",
   "metadata": {},
   "source": [
    "# Define RAG System Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8de4a4-0058-44fc-b7b5-ce222b69d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGSystem class defined\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Production-grade RAG system with indexing and query capabilities.\n",
    "    Combines document loading, chunking, embedding, retrieval, re-ranking, and generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize RAG system components.\"\"\"\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.cross_encoder = None\n",
    "        self.llm = None\n",
    "        \n",
    "        logger.info(\"RAGSystem initialized\")\n",
    "\n",
    "        \n",
    "    \n",
    "    # ==================== INDEXING PHASE ====================\n",
    "    # Document loading function (required by RAGSystem)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load(source: str) -> List[Document]:\n",
    "        \"\"\"Auto-detect and load file, directory, or web URL using LangChain loaders.\"\"\"\n",
    "        \n",
    "        # Check web URL first\n",
    "        if source.startswith((\"http://\", \"https://\")):\n",
    "            loader = WebBaseLoader(source)\n",
    "            return loader.load()\n",
    "        \n",
    "        # Check file/directory\n",
    "        source_path = Path(source)\n",
    "        \n",
    "        if source_path.is_file():\n",
    "            loader = UnstructuredFileLoader(str(source_path))\n",
    "            return loader.load()\n",
    "            \n",
    "        elif source_path.is_dir():\n",
    "            loader = DirectoryLoader(\n",
    "                str(source_path),\n",
    "                glob=\"**/*\",\n",
    "                loader_cls=UnstructuredFileLoader,\n",
    "                use_multithreading=True\n",
    "            )\n",
    "            return loader.load()\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid source: {source}\")\n",
    "    \n",
    "    def load_documents(self, source: str) -> None:\n",
    "        \"\"\"\n",
    "        Load documents from file, directory, or URL.\n",
    "        \n",
    "        Args:\n",
    "            source: Path to file/directory or URL\n",
    "        \"\"\"\n",
    "        docs = RAGSystem._load(source)  # Call static method\n",
    "        self.documents.extend(docs)\n",
    "        logger.info(f\"Loaded {len(docs)} documents from {source}\")\n",
    "        print(f\"Loaded {len(docs)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def create_chunks(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> None:\n",
    "        \"\"\"\n",
    "        Split documents into chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Size of each chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"; \", \" \", \"\"],\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        \n",
    "        self.chunks = text_splitter.split_documents(self.documents)\n",
    "        logger.info(f\"Created {len(self.chunks)} chunks\")\n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(self.documents)} documents\")\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "        \"\"\"\n",
    "        Create embeddings and build vector store index.\n",
    "        Initializes embeddings, cross-encoder, LLM, and vector store.\n",
    "        \"\"\"\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No chunks available. Call create_chunks() first.\")\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'mps'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        logger.info(\"Embeddings model loaded\")\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=\"rag_knowledge_base\"\n",
    "        )\n",
    "        logger.info(f\"Vector store created with {len(self.chunks)} chunks\")\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 10}\n",
    "        )\n",
    "        logger.info(\"Retriever initialized\")\n",
    "        \n",
    "        # Initialize cross-encoder for re-ranking\n",
    "        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        logger.info(\"Cross-encoder loaded\")\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            temperature=0.3,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        logger.info(\"LLM initialized\")\n",
    "        \n",
    "        print(\"Index built successfully!\")\n",
    "        print(f\"  - {len(self.chunks)} chunks indexed\")\n",
    "        print(f\"  - Embeddings: all-MiniLM-L6-v2\")\n",
    "        print(f\"  - Cross-encoder: ms-marco-MiniLM-L-6-v2\")\n",
    "        print(f\"  - LLM: gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "        \n",
    "    # ==================== QUERY PHASE ====================\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 10) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k candidate documents.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved documents\n",
    "        \"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        results = self.retriever.invoke(query)\n",
    "        logger.info(f\"Retrieved {len(results)} candidates\")\n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = 3) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Re-rank documents using cross-encoder.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            documents: List of candidate documents\n",
    "            top_k: Number of top documents to return\n",
    "            \n",
    "        Returns:\n",
    "            Top-k re-ranked documents\n",
    "        \"\"\"\n",
    "        if not self.cross_encoder:\n",
    "            raise ValueError(\"Cross-encoder not initialized. Call build_index() first.\")\n",
    "        \n",
    "        pairs = [[query, doc.page_content] for doc in documents]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_docs = [doc for doc, score in scored_docs[:top_k]]\n",
    "        logger.info(f\"Re-ranked to top {top_k} documents\")\n",
    "        \n",
    "        return top_docs\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using LLM with retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context_docs: Retrieved and re-ranked documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            raise ValueError(\"LLM not initialized. Call build_index() first.\")\n",
    "        \n",
    "        # Format context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document {i+1}]\\n{doc.page_content}\" \n",
    "            for i, doc in enumerate(context_docs)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_template = \"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the information in the context\n",
    "- If the context doesn't contain enough information, say \"I don't have enough information to answer that.\"\n",
    "- Be concise and accurate\n",
    "- Use natural language\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        messages = prompt.format_messages(context=context, question=query)\n",
    "        \n",
    "        # Generate answer\n",
    "        response = self.llm.invoke(messages)\n",
    "        logger.info(\"Answer generated\")\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool = False) -> str | Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline: retrieve -> re-rank -> generate.\n",
    "        This is the main public API for querying the system.\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            return_sources: If True, return dict with answer and source documents\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer (string) or dict with answer and sources\n",
    "        \"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"System not ready. Call build_index() first.\")\n",
    "        \n",
    "        logger.info(f\"Query: {question}\")\n",
    "        \n",
    "        # Step 1: Retrieve candidates\n",
    "        candidates = self.retrieve(question, k=10)\n",
    "        \n",
    "        # Step 2: Re-rank to top 3\n",
    "        top_docs = self.rerank(question, candidates, top_k=3)\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        answer = self.generate(question, top_docs)\n",
    "        \n",
    "        if return_sources:\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": top_docs\n",
    "            }\n",
    "        else:\n",
    "            return answer\n",
    "\n",
    "\n",
    "        \n",
    "    # ==================== UTILITY METHODS ====================\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system statistics.\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"index_built\": self.vectorstore is not None,\n",
    "        }\n",
    "\n",
    "print(\"RAGSystem class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c0f89-23e3-48ca-b769-e02932e5c9fb",
   "metadata": {},
   "source": [
    "# Initialize and Build RAG System\n",
    "- Create RAG system instance\n",
    "- Create chunks\n",
    "- Build index (Embeddings, vector store, retriever and LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70705339-9466-4b58-88ee-c2ccf01c3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 09:58:33 - IndustrialRAG - INFO - RAGSystem initialized\n",
      "2026-02-09 09:58:33 - unstructured - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-02-09 09:58:34 - IndustrialRAG - INFO - Loaded 3 documents from test_documents\n",
      "2026-02-09 09:58:34 - IndustrialRAG - INFO - Loaded 1 documents from https://karpathy.ai/\n",
      "2026-02-09 09:58:34 - IndustrialRAG - INFO - Document analysis: 21618 tokens, strategy=large\n",
      "2026-02-09 09:58:34 - IndustrialRAG - INFO - Created 80 chunks\n",
      "2026-02-09 09:58:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Loaded 3 documents. Total: 3\n",
      "Loaded 1 documents. Total: 4\n",
      "Created 80 chunks from 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 09:58:34 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:34 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-09 09:58:35 - huggingface_hub.utils._http - WARNING - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:35 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2980da351b4af98a7aadc6267dc8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:36 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:37 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:37 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:37 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 09:58:37 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:38 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:38 - IndustrialRAG - INFO - Embeddings model loaded\n",
      "2026-02-09 09:58:39 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2026-02-09 09:58:40 - IndustrialRAG - INFO - Vector store created with 80 chunks\n",
      "2026-02-09 09:58:40 - IndustrialRAG - INFO - Retriever initialized\n",
      "2026-02-09 09:58:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a68ff4265e4de79de7a3d3a0c5d290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:42 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:42 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 09:58:42 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:42 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:43 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:43 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:43 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:43 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: mps\n",
      "2026-02-09 09:58:43 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2 \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 09:58:44 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:58:44 - IndustrialRAG - INFO - Cross-encoder loaded\n",
      "2026-02-09 09:58:44 - google_genai._api_client - WARNING - Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "2026-02-09 09:58:44 - IndustrialRAG - INFO - LLM initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built successfully!\n",
      "  - 80 chunks indexed\n",
      "  - Embeddings: all-MiniLM-L6-v2\n",
      "  - Cross-encoder: ms-marco-MiniLM-L-6-v2\n",
      "  - LLM: gemini-2.5-flash\n",
      "\n",
      "RAG System ready for queries!\n"
     ]
    }
   ],
   "source": [
    "# Create RAG system instance\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Load documents\n",
    "rag.load_documents(\"test_documents\")\n",
    "rag.load_documents(\"https://karpathy.ai/\")\n",
    "\n",
    "# Create chunks (using adaptive sizing based on total content)\n",
    "combined_text = \"\\n\\n\".join([doc.page_content for doc in rag.documents])\n",
    "analysis = analyze_document(combined_text)\n",
    "\n",
    "rag.create_chunks(\n",
    "    chunk_size=analysis['suggested_chunk_size'],\n",
    "    chunk_overlap=analysis['suggested_overlap']\n",
    ")\n",
    "\n",
    "# Build index (embeddings, vector store, retriever, cross-encoder, LLM)\n",
    "rag.build_index()\n",
    "\n",
    "print(\"\\nRAG System ready for queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a4932-670f-4dab-8112-eed2ffc68b75",
   "metadata": {},
   "source": [
    "# Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9949199-cf63-4c27-9e14-00caffee6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 09:58:56 - IndustrialRAG - INFO - Query: How does retrieval augmented generation work?\n",
      "2026-02-09 09:58:56 - IndustrialRAG - INFO - Retrieved 10 candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Question: How does retrieval augmented generation work?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6465a29482c4100a66c69e2c046d0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 09:58:57 - IndustrialRAG - INFO - Re-ranked to top 3 documents\n",
      "2026-02-09 09:58:57 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-02-09 09:59:00 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:59:00 - IndustrialRAG - INFO - Answer generated\n",
      "2026-02-09 09:59:00 - IndustrialRAG - INFO - Query: What is Zara Zhang's email address?\n",
      "2026-02-09 09:59:00 - IndustrialRAG - INFO - Retrieved 10 candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Retrieval-Augmented Generation (RAG) works through several key steps:\n",
      "1.  **Document Ingestion**: Documents are loaded and processed.\n",
      "2.  **Chunking**: Large documents are split into smaller chunks.\n",
      "3.  **Embedding Generation**: Each chunk is converted into a dense vector representation using embedding models.\n",
      "4.  **Vector Storage**: Embeddings are stored in a vector database.\n",
      "5.  **Query Processing**: A user's question is converted into an embedding.\n",
      "6.  **Retrieval**: The system finds the most relevant chunks from the vector database using similarity search.\n",
      "7.  **Context Augmentation**: Retrieved chunks are added to the prompt as context.\n",
      "8.  **Generation**: A large language model (LLM) generates a response based on the question and the retrieved context.\n",
      "\n",
      "Sources used:\n",
      "  1. rag_introduction.txt\n",
      "  2. rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "  3. rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: What is Zara Zhang's email address?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323ec14418664f049675fb24aea00563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 09:59:00 - IndustrialRAG - INFO - Re-ranked to top 3 documents\n",
      "2026-02-09 09:59:00 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-02-09 09:59:02 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 09:59:02 - IndustrialRAG - INFO - Answer generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "Zara Zhang's email address is zara.r.zhang@gmail.com.\n",
      "\n",
      "Sources used:\n",
      "  1. About-Zara-Zhang.docx\n",
      "  2. rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "  3. rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"How does retrieval augmented generation work?\",\n",
    "    \"What is Zara Zhang's email address?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    result = rag.query(question, return_sources=True)\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "    print(f\"\\nSources used:\")\n",
    "    \n",
    "    for i, doc in enumerate(result['sources'], 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        print(f\"  {i}. {Path(source).name if not source.startswith('http') else source}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b82a5-7d9b-4ab4-902f-9c4f049f5597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-demo)",
   "language": "python",
   "name": "rag-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
