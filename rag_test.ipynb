{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924d4e1a-45ce-49f0-abe5-6eed8bf9f29b",
   "metadata": {},
   "source": [
    "# **Phase 0: Setting Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f95795-6b78-4e5e-9563-5ddab2b86523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ivantan/Desktop/rag-demo/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75db8c4-0abd-4282-abbc-5692b832a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries imported successfully!\n",
      "Timestamp: 2026-02-08 23:49:52\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv # Environment and configuration\n",
    "\n",
    "# LangChain core\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import Chroma # Vector store\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # LLM\n",
    "from rank_bm25 import BM25Okapi # BM25 for hybrid search\n",
    "import tiktoken # Token counting\n",
    "\n",
    "print(\"Core libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2218334-54b8-4d85-a766-612863093829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:52 - IndustrialRAG - INFO - Logging system initialized\n",
      "2026-02-08 23:49:52 - IndustrialRAG - INFO - Starting Industrial RAG System Development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# This helps track the RAG pipeline execution and identify bottlenecks\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('IndustrialRAG')\n",
    "\n",
    "# Test logging\n",
    "logger.info(\"Logging system initialized\")\n",
    "logger.info(\"Starting Industrial RAG System Development\")\n",
    "print(\"Logging configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f138b84-2bd7-484f-8b57-daf57f11e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:52 - IndustrialRAG - INFO - API Key loaded: AIzaSyAU...LT-U\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key loaded successfully!\n",
      "Masked Key: AIzaSyAU...LT-U\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in .env file!\")\n",
    "\n",
    "# Mask the key for security when logging\n",
    "masked_key = f\"{GOOGLE_API_KEY[:8]}...{GOOGLE_API_KEY[-4:]}\"\n",
    "logger.info(f\"API Key loaded: {masked_key}\")\n",
    "\n",
    "print(f\"Google API Key loaded successfully!\")\n",
    "print(f\"Masked Key: {masked_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401899a1-2ed6-4d37-bc40-7b19efd45b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses tiktoken (OpenAI's tokenizer) as a proxy for token estimation\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        tokens = encoding.encode(text)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error counting tokens: {e}\")\n",
    "        # Fallback: rough estimation (1 token ≈ 4 characters)\n",
    "        return len(text) // 4\n",
    "\n",
    "def analyze_document(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"   \n",
    "    Strategy:\n",
    "    - Short docs (<2000 tokens): chunk_size=500, overlap=100\n",
    "    - Medium docs (2000-10000 tokens): chunk_size=1000, overlap=200\n",
    "    - Long docs (>10000 tokens): chunk_size=1500, overlap=300\n",
    "    Returns: Dictionary with token count and suggested chunk parameters\n",
    "    \"\"\"\n",
    "    token_count = count_tokens(text)\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    \n",
    "    # Determine optimal chunk size based on document length\n",
    "    if token_count < 2000:\n",
    "        chunk_size = 500\n",
    "        chunk_overlap = 100\n",
    "        strategy = \"small\"\n",
    "    elif token_count < 10000:\n",
    "        chunk_size = 1000\n",
    "        chunk_overlap = 200\n",
    "        strategy = \"medium\"\n",
    "    else:\n",
    "        chunk_size = 1500\n",
    "        chunk_overlap = 300\n",
    "        strategy = \"large\"\n",
    "    \n",
    "    analysis = {\n",
    "        \"token_count\": token_count,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"suggested_chunk_size\": chunk_size,\n",
    "        \"suggested_overlap\": chunk_overlap,\n",
    "        \"strategy\": strategy\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Document analysis: {token_count} tokens, strategy={strategy}\")\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335675f8-c970-41e3-95f9-e063f234e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:52 - IndustrialRAG - INFO - Document analysis: 59 tokens, strategy=small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counter utility created!\n",
      "\n",
      "Test Document Analysis:\n",
      "    Tokens: 59\n",
      "    Words: 44\n",
      "    Characters: 326\n",
      "    Suggested chunk size: 500\n",
      "    Suggested overlap: 100\n",
      "    Strategy: small\n"
     ]
    }
   ],
   "source": [
    "# Test the token counter\n",
    "test_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming the world at an unprecedented pace.\n",
    "Machine learning, a subset of AI, enables computers to learn from data without\n",
    "explicit programming. Deep learning, using neural networks, has achieved remarkable\n",
    "results in computer vision, natural language processing, and speech recognition.\n",
    "\"\"\"\n",
    "\n",
    "analysis = analyze_document(test_text)\n",
    "\n",
    "print(\"Token counter utility created!\")\n",
    "print(f\"\\nTest Document Analysis:\")\n",
    "print(f\"    Tokens: {analysis['token_count']}\")\n",
    "print(f\"    Words: {analysis['word_count']}\")\n",
    "print(f\"    Characters: {analysis['char_count']}\")\n",
    "print(f\"    Suggested chunk size: {analysis['suggested_chunk_size']}\")\n",
    "print(f\"    Suggested overlap: {analysis['suggested_overlap']}\")\n",
    "print(f\"    Strategy: {analysis['strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e06ad-51bc-4c1f-8100-0239d4030624",
   "metadata": {},
   "source": [
    "# **Phase 2: Document Processing Pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c2245c-0763-4a35-85e6-dd9c5d585c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:52 - langchain_community.utils.user_agent - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader, WebBaseLoader\n",
    "\n",
    "def load(source: str) -> List[Document]:\n",
    "    \"\"\"Auto-detect and load file or directory using LangChain loaders.\"\"\"\n",
    "    \n",
    "    if source.startswith((\"http://\", \"https://\")):\n",
    "        loader = WebBaseLoader(source)\n",
    "        return loader.load()\n",
    "    \n",
    "    source_path = Path(source)\n",
    "    \n",
    "    if source_path.is_file():\n",
    "        loader = UnstructuredFileLoader(str(source_path))\n",
    "        return loader.load()\n",
    "        \n",
    "    elif source_path.is_dir():\n",
    "        loader = DirectoryLoader(\n",
    "            str(source_path),\n",
    "            glob=\"**/*\",\n",
    "            loader_cls=UnstructuredFileLoader,\n",
    "            use_multithreading=True\n",
    "        )\n",
    "        return loader.load()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source: {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f782554-1f49-4aeb-aa33-ab785f98a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:54 - unstructured - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-02-08 23:49:55 - pikepdf._core - INFO - pikepdf C++ to Python logger bridge initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "dir_doc = load(\"test_documents\")\n",
    "web_doc = load(\"https://karpathy.ai/\")\n",
    "all_doc = dir_doc + web_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a85801-4f96-4550-93ed-cb15b943570d",
   "metadata": {},
   "source": [
    "# **Phase 3: Chunking and Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1862e3-1ee8-4bb3-a8bf-0405d4f18fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Chunking - Using LangChain's RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks(documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    Tries to split on paragraphs, then sentences, maintaining semantic coherence.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \";\", \" \", \"\"],\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    logger.info(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7c9300-6667-4357-9812-88efafb58b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:56 - IndustrialRAG - INFO - Split 4 documents into 113 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 113 chunks from 4 documents\n",
      "\n",
      "\n",
      "First 100 chars in the first doc:\n",
      " # Retrieval\n",
      "\n",
      "Augmented Generation (RAG) Systems\n",
      "\n",
      "## Introduction Retrieval-Augmented Generation (RAG\n",
      "\n",
      "First 100 chars in the second doc:\n",
      " 5. **Query Processing**: When a user asks a question, it's converted into an embedding using the sam\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = create_chunks(all_doc)\n",
    "print(f\"Created {len(all_chunks)} chunks from {len(all_doc)} documents\\n\")\n",
    "print(\"\\nFirst 100 chars in the first doc:\\n\", all_chunks[0].page_content[:100])\n",
    "print(\"\\nFirst 100 chars in the second doc:\\n\", all_chunks[1].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ada52e-f92f-4d54-83ea-825640fe84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:49:56 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2026-02-08 23:49:56 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-08 23:49:56 - huggingface_hub.utils._http - WARNING - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-08 23:49:56 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:57 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19470b1ccbe54b019b697c6b171afd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:58 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-08 23:49:59 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:50:00 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n",
      "2026-02-08 23:50:00 - IndustrialRAG - INFO - Embeddings model initialized: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model ready (384 dimensions)\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Embeddings - Using HuggingFace sentence-transformers\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'mps'},  # Use Apple Silicon GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "logger.info(\"Embeddings model initialized: all-MiniLM-L6-v2\")\n",
    "print(\"Embeddings model ready (384 dimensions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb56071-2ae8-44ec-ad69-c661c8642140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:50:01 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2026-02-08 23:50:02 - IndustrialRAG - INFO - Vector store created with 113 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store indexed: 113 chunks ready for retrieval\n"
     ]
    }
   ],
   "source": [
    "# Vector Store - Index all chunks in ChromaDB\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_knowledge_base\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Vector store created with {len(all_chunks)} chunks\")\n",
    "print(f\"Vector store indexed: {len(all_chunks)} chunks ready for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377d0e0-68cd-4d12-a686-6187c7d1afd8",
   "metadata": {},
   "source": [
    "# **Phase 4: Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a636f6d-8a7b-405e-8ffe-e182cf735feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 23:50:02 - IndustrialRAG - INFO - Retriever created with k=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready: will fetch top 10 candidates\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Retrieval - Create retriever from vector store\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}  # Retrieve top 10 candidates for re-ranking\n",
    ")\n",
    "\n",
    "logger.info(\"Retriever created with k=10\")\n",
    "print(\"Retriever ready: will fetch top 10 candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8f76c-6076-4318-97ad-8a37fa0bb2c6",
   "metadata": {},
   "source": [
    "## Retrieval Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4fc03e3-5b76-47ee-a4da-84f79aaadc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the benefits of RAG systems?'\n",
      "Retrieved 10 documents\n",
      "\n",
      "Result [1]:\n",
      "9  Broader Impact  This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual k...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [2]:\n",
      ". Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned....\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [3]:\n",
      "Acknowledgments  The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace fo...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [4]:\n",
      "Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [5]:\n",
      "4.3  Jeopardy Question Generation  Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models ou...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_k_snapshot(query: str, results: List[Document], top_k: int = 5) -> None:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Retrieved {len(results)} documents\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results[:top_k], 1):\n",
    "        print(f\"Result [{i}]:\")\n",
    "        print(f\"{doc.page_content[:150].replace('\\n', ' ')}...\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
    "\n",
    "retrieval_results = retriever.invoke(test_query) # This is where the actual retrieval happens - executes the search and returns the actual documents\n",
    "top_k_snapshot(test_query, retrieval_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a0944-721d-4552-81dc-0d40d5d960f9",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "206c2598-f5f0-43bd-9779-cbea2e5addb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:33 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:33 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:33 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:33 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49a680c131c42deb6a8653ae37b5c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:34 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/model.safetensors \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:34 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "2026-02-09 08:34:34 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2/xet-read-token/c5ee24cb16019beea0893ab7796b1df96625c6b8 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e838be7814653809c822902540a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20b5ccff0d94398a744c56529c96089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:38 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer_config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b137f10636424396d58c7b23327ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:39 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:39 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 08:34:39 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:39 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/vocab.txt \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/vocab.txt \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/vocab.txt \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/vocab.txt \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50728175997f42ba946d372c5bb6d0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:40 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f560d19477f843fba936398d3a1636e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/added_tokens.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:41 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/special_tokens_map.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db677960b9bb421588c327c3c91a7728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/chat_template.jinja \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:34:42 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/README.md \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52abc99e1b5d4a739287741eef8575eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:34:42 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: mps\n",
      "2026-02-09 08:34:43 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L-6-v2 \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-09 08:34:43 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cross-encoder/ms-marco-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder re-ranker initialized\n",
      "Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
     ]
    }
   ],
   "source": [
    "# Re-ranking with Cross-Encoder (production approach)\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize cross-encoder model for re-ranking\n",
    "# This model is specifically trained to score query-document pairs\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def rerank_documents(query: str, documents: List[Document], top_k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using cross-encoder.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        documents: List of retrieved documents\n",
    "        top_k: Number of top documents to return after re-ranking\n",
    "    \n",
    "    Returns:\n",
    "        Top-k re-ranked documents\n",
    "    \"\"\"\n",
    "    # Create query-document pairs for scoring\n",
    "    pairs = [[query, doc.page_content] for doc in documents]\n",
    "    \n",
    "    # Get relevance scores from cross-encoder\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort documents by score (descending)\n",
    "    scored_docs = list(zip(documents, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k documents\n",
    "    top_docs = [doc for doc, score in scored_docs[:top_k]]\n",
    "    \n",
    "    logger.info(f\"Re-ranked {len(documents)} docs to top {top_k}\")\n",
    "    \n",
    "    return top_docs\n",
    "\n",
    "print(\"Cross-encoder re-ranker initialized\")\n",
    "print(\"Model: cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0753effc-60f2-49a4-b025-50a425d69e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b744ff73e9141b2847849b966f4699e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:44:25 - IndustrialRAG - INFO - Re-ranked 10 docs to top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the benefits of RAG systems?'\n",
      "Retrieved 3 documents\n",
      "\n",
      "Result [1]:\n",
      "With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual an...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [2]:\n",
      "9  Broader Impact  This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual k...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n",
      "Result [3]:\n",
      "4.4 Fact Veriﬁcation  Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are co...\n",
      "Source: test_documents/rag-for-knowledge-intensive-nlp-tasks.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rerank_results = rerank_documents(test_query, retrieval_results)\n",
    "top_k_snapshot(test_query, rerank_results, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2703bd5-bba6-4964-8efd-3e2476da72cf",
   "metadata": {},
   "source": [
    "# **Phase 5: Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2948a0f8-1987-47a2-96ee-34c429e580a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:49:12 - google_genai._api_client - WARNING - Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "2026-02-09 08:49:12 - IndustrialRAG - INFO - Gemini LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# LLM Integration - Initialize Gemini for answer generation\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "logger.info(\"Gemini LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61fb2086-7d87-404d-885b-4b6d88086299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Prompt - Define how to combine context with query\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt_template = \"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based ONLY on the information in the context\n",
    "- If the context doesn't contain enough information, say \"I don't have enough information to answer that.\"\n",
    "- Be concise and accurate\n",
    "- Use natural language\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aef3a7d1-4d46-487c-837a-d1a5eb894b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: RAG Chain - Combine retrieval, re-ranking, and generation\n",
    "\n",
    "def generate_answer(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve -> re-rank -> generate answer.\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer as string\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve top 10 candidates\n",
    "    candidates = retriever.invoke(query)\n",
    "    logger.info(f\"Retrieved {len(candidates)} candidates\")\n",
    "    \n",
    "    # Step 2: Re-rank to top 3\n",
    "    top_docs = rerank_documents(query, candidates, top_k=3)\n",
    "    logger.info(f\"Re-ranked to top {len(top_docs)} documents\")\n",
    "    \n",
    "    # Step 3: Format context from top documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Document {i+1}]\\n{doc.page_content}\" \n",
    "        for i, doc in enumerate(top_docs)\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Create prompt with context and question\n",
    "    messages = rag_prompt.format_messages(context=context, question=query)\n",
    "    \n",
    "    # Step 5: Generate answer with LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    logger.info(\"Answer generated\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d0853eb-d843-48cc-8eb4-0852563e1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Question: What are the benefits of RAG systems?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:51:34 - IndustrialRAG - INFO - Retrieved 10 candidates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba36e4c1f6054a768b4d8d790e1d7674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:51:34 - IndustrialRAG - INFO - Re-ranked 10 docs to top 3\n",
      "2026-02-09 08:51:34 - IndustrialRAG - INFO - Re-ranked to top 3 documents\n",
      "2026-02-09 08:51:34 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-02-09 08:51:37 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:51:37 - IndustrialRAG - INFO - Answer generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "RAG systems offer several benefits: they are more strongly grounded in real factual knowledge, leading to less \"hallucination\" and more factual generations. They also provide more control and interpretability. RAG can be employed in various scenarios to benefit society, such as answering open-domain medical questions or helping people be more effective at their jobs. Additionally, RAG scores are competitive with state-of-the-art models without requiring complex pipeline systems, domain-specific architectures, substantial engineering, or intermediate retrieval supervision.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question: How does RAG work?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:51:38 - IndustrialRAG - INFO - Retrieved 10 candidates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18afb739d9be4b3083f966614f74c83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 08:51:38 - IndustrialRAG - INFO - Re-ranked 10 docs to top 3\n",
      "2026-02-09 08:51:38 - IndustrialRAG - INFO - Re-ranked to top 3 documents\n",
      "2026-02-09 08:51:38 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.\n",
      "2026-02-09 08:51:40 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2026-02-09 08:51:40 - IndustrialRAG - INFO - Answer generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "RAG works by combining information retrieval with language generation. The process involves several steps: documents are ingested and processed, then split into smaller chunks. Each chunk is converted into a dense vector representation using embedding models, and these embeddings are stored in a vector database. When a user asks a question, it is also converted into an embedding.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test complete RAG system\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the benefits of RAG systems?\",\n",
    "    \"How does RAG work?\",\n",
    "    # \"What are the challenges in RAG?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    answer = generate_answer(question)\n",
    "    \n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    print(\"=\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826d7d9-f269-434d-9376-7762ee4da70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-demo)",
   "language": "python",
   "name": "rag-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
